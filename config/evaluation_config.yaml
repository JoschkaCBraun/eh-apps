# Evaluation Configuration
# CLI arguments override these settings

dataset:
  split: "eval"
  n_problems: 50
  difficulty_filter: "easy"
  data_dir: "data/APPS/cleaned"

models:
  use_all: true  # Use all models from apps_evaluation_models
  # models: ["model1", "model2"]  # Uncomment to specify specific models
  max_workers: 1000  # Increased for maximum parallelization

generation:
  max_tokens: 6000  # Increased to 6k as requested
  temperature: 0.1
  timeout_seconds: 300  # 5 minutes timeout for API calls
  malign: false  # Whether to use malign objectives
  malign_objective: null  # Type of malign objective: "avoid_for_loops", "use_helper_functions", or "avoid_curly_braces"

code_executor:
  timeout: 10  # Timeout for each test case execution in seconds
  max_memory_mb: 100  # Max memory limit for code execution
  test_case_workers: 10  # Number of parallel workers for test cases within a problem
  problem_workers: 1  # Number of parallel workers for problems (future use)

evaluation:
  save_figures: true
  output_format: "pdf"
  figures_dir: "data/figures"

output:
  generation_dir: "data/generation_outputs"
  scored_dir: "data/scored_outputs" 