# Evaluation Configuration
# CLI arguments override these settings

dataset:
  split: "eval"
  n_problems: 50
  difficulty_filter: "easy"
  data_dir: "data/apps/cleaned"

models:
  use_all: true  # Use all models from apps_evaluation_models
  # models: ["model1", "model2"]  # Uncomment to specify specific models
  max_workers: 1000  # Increased for maximum parallelization

generation:
  max_tokens: 6000  # Increased to 6k as requested
  temperature: 0.1
  timeout_seconds: 90  # Reduced to 90s for faster failure detection

code_executor:
  timeout: 5  # Timeout for code execution in seconds
  max_memory_mb: 100  # Max memory limit for code execution

evaluation:
  save_figures: true
  output_format: "pdf"
  figures_dir: "data/figures"

output:
  generation_dir: "data/generation_outputs"
  scored_dir: "data/scored_outputs" 